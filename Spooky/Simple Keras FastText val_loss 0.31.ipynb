{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "73969602-c633-450e-9364-9481a21a879c",
    "_uuid": "5484dedb6db529f4dc58d0cb433186b3d823f429"
   },
   "source": [
    "# **This notebook's best result: val_acc is 0.8779, val_loss is 0.3129**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d5736ce6-a0b0-4cf0-beaf-a73837398da9",
    "_uuid": "7ccc3b4516a4fcde346a162ae4f9461016bbfbbf"
   },
   "source": [
    "# **1. Few Preprocessings**\n",
    "# **2. Model: FastText by Keras**\n",
    "## **2.1** Change Preprocessings:\n",
    "- Do lower case "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "93e00783-a024-4e87-a5e1-6709cb8cc981",
    "_kg_hide-input": false,
    "_kg_hide-output": true,
    "_uuid": "b05ef71268db76a4e2565177bf6a5668a5fc428e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.layers import Dense, GlobalAveragePooling1D, Embedding\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "a5cc2c3e-7960-482e-b548-c447b89925ec",
    "_uuid": "d700f739101e37903112e1de293323dcfbb577be",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('input/train.csv')\n",
    "a2c = {'EAP': 0, 'HPL' : 1, 'MWS' : 2}\n",
    "y = np.array([a2c[a] for a in df.author])\n",
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a45cb3ba-d1bc-48e0-956c-27d0f49a9943",
    "_uuid": "a01bab31ed7b8a55820612063576963488d99eb6"
   },
   "source": [
    "# 1. **Few Preprocessings**\n",
    "\n",
    "In traditional NLP tasks, preprocessings play an important role, but...\n",
    "\n",
    "## **Low-frequency words**\n",
    "In my experience, fastText is very fast, but I need to delete rare words to avoid overfitting.\n",
    "\n",
    "**NOTE**:\n",
    "Some keywords are rare words, such like *Cthulhu* in *Cthulhu Mythos* of *Howard Phillips Lovecraft*.\n",
    "But these are useful for this task.\n",
    "\n",
    "## **Removing Stopwords**\n",
    "\n",
    "Nothing.\n",
    "To identify author from a sentence, some stopwords play an important role because one has specific usages of them.\n",
    "\n",
    "## **Stemming and Lowercase**\n",
    "\n",
    "Nothing.\n",
    "This reason is the same for stopwords removing.\n",
    "And I guess some stemming rules provided by libraries is bad for this task because all author is the older author.\n",
    "\n",
    "## **Cutting long sentence**\n",
    "\n",
    "Too long documents are cut.\n",
    "\n",
    "## **Punctuation**\n",
    "\n",
    "Because I guess each author has unique punctuations's usage in the novel, I separate them from words.\n",
    "\n",
    "e.g. `Don't worry` -> `Don ' t worry`\n",
    "\n",
    "## **Is it slow?**\n",
    "\n",
    "Don't worry! FastText is a very fast algorithm if it runs on CPU. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8182b25a-f490-4b41-9865-ee1c04afecee",
    "_uuid": "0023cd1542d866d931deb8472f8a0d6fb0262d9a"
   },
   "source": [
    "# **Let's check character distribution per author**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "c1d00b0d-90e0-4f19-842c-51a82de42a10",
    "_kg_hide-output": true,
    "_uuid": "246a428ca3a063294c15c8c08d234ecf01e4ddbb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c HPL   MWS   EAP   \n",
      "L 249 307 458 \n",
      "N 345 204 411 \n",
      "C 439 308 395 \n",
      "Q 10 7 21 \n",
      "G 318 246 313 \n",
      "s 43915 45962 53841 \n",
      "x 1061 1267 1951 \n",
      "à 0 0 10 \n",
      "Π 1 0 0 \n",
      "W 732 681 739 \n",
      "F 269 232 383 \n",
      "î 0 0 1 \n",
      "e 88259 97515 114885 \n",
      ": 47 339 176 \n",
      "u 19519 21025 26311 \n",
      "f 16272 18351 22354 \n",
      "t 62235 63142 82426 \n",
      "I 3480 4917 4846 \n",
      "; 1143 2662 1354 \n",
      "c 18338 17911 24127 \n",
      "k 5204 3707 4277 \n",
      "M 645 415 1065 \n",
      "U 94 46 166 \n",
      "Y 111 234 282 \n",
      "v 6529 7948 9624 \n",
      "ô 0 0 8 \n",
      "α 2 0 0 \n",
      "O 503 282 414 \n",
      "R 237 385 258 \n",
      "ñ 7 0 0 \n",
      "i 44250 46080 60952 \n",
      "è 0 0 15 \n",
      "d 33366 35315 36862 \n",
      "Ν 1 0 0 \n",
      "æ 10 0 36 \n",
      "ä 6 0 1 \n",
      "Υ 1 0 0 \n",
      "B 533 395 835 \n",
      "Å 1 0 0 \n",
      "δ 2 0 0 \n",
      "H 741 669 864 \n",
      "m 17622 20471 22792 \n",
      "y 12534 14877 17001 \n",
      "V 67 57 156 \n",
      "ë 12 0 0 \n",
      "K 176 35 86 \n",
      "Æ 4 0 1 \n",
      "Z 51 2 23 \n",
      "h 42770 43738 51580 \n",
      "n 50879 50291 62636 \n",
      "J 210 66 164 \n",
      "w 15554 16062 17507 \n",
      "ç 0 0 1 \n",
      "q 779 677 1030 \n",
      "\" 513 1469 2987 \n",
      "r 40590 44042 51221 \n",
      "ê 2 0 28 \n",
      "? 169 419 510 \n",
      "l 30273 27819 35371 \n",
      "' 1710 476 1334 \n",
      "â 0 0 6 \n",
      "b 10636 9611 13245 \n",
      "o 50996 53386 67145 \n",
      "A 1167 943 1258 \n",
      "ï 7 0 0 \n",
      "g 14951 12601 16088 \n",
      "a 56815 55274 68525 \n",
      "X 5 4 17 \n",
      "p 10965 12361 17422 \n",
      "ö 3 0 16 \n",
      "P 320 365 442 \n",
      ". 5908 5761 8406 \n",
      "j 424 682 683 \n",
      "ü 5 0 1 \n",
      "z 529 400 634 \n",
      "é 15 0 47 \n",
      "D 334 227 491 \n",
      "T 1583 1230 2217 \n",
      "Σ 1 0 0 \n",
      ", 8581 12045 17594 \n",
      "Ο 3 0 0 \n",
      "E 281 445 435 \n",
      "ἶ 2 0 0 \n",
      "S 841 578 729 \n"
     ]
    }
   ],
   "source": [
    "counter = {name : defaultdict(int) for name in set(df.author)}\n",
    "for (text, author) in zip(df.text, df.author):\n",
    "    text = text.replace(' ', '')\n",
    "    for c in text:\n",
    "        counter[author][c] += 1\n",
    "\n",
    "chars = set()\n",
    "for v in counter.values():\n",
    "    chars |= v.keys()\n",
    "    \n",
    "names = [author for author in counter.keys()]\n",
    "\n",
    "print('c ', end='')\n",
    "for n in names:\n",
    "    print(n, end='   ')\n",
    "print()\n",
    "for c in chars:    \n",
    "    print(c, end=' ')\n",
    "    for n in names:\n",
    "        print(counter[n][c], end=' ')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7a3fdf4e-039d-4c93-bc21-9bad7dfc6ff8",
    "_uuid": "8e72d6f22587780364ed24cae13ece4a403479dd"
   },
   "source": [
    "# **Summary of character distribution**\n",
    "\n",
    "- HPL and EAP used non ascii characters like a `ä`.\n",
    "- The number of punctuations seems to be good feature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ce97fc0a-b85c-4f34-92c5-ae66a0730ace",
    "_uuid": "fee49fd9139b78ae03603d7d37eafa38f3cb29dc"
   },
   "source": [
    "# **Preprocessing**\n",
    "\n",
    "My preproceeings are \n",
    "\n",
    "- Separate punctuation from words\n",
    "- Remove lower frequency words ( <= 2)\n",
    "- Cut a longer document which contains `256` words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "72ff2ff5-0945-4f39-8b02-39e4d5df16c5",
    "_uuid": "999012010cd8b9b20d3c5b16c11a2374a5ce44c0",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.replace(\"' \", \" ' \")\n",
    "    signs = set(',.:;\"?!')\n",
    "    prods = set(text) & signs\n",
    "    if not prods:\n",
    "        return text\n",
    "\n",
    "    for sign in prods:\n",
    "        text = text.replace(sign, ' {} '.format(sign) )\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "f123742f-540f-438d-aba3-ebbca69235be",
    "_uuid": "53f325a090a44f7109f0537022398797704cdc80",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_docs(df, n_gram_max=2):\n",
    "    def add_ngram(q, n_gram_max):\n",
    "            ngrams = []\n",
    "            for n in range(2, n_gram_max+1):\n",
    "                for w_index in range(len(q)-n+1):\n",
    "                    ngrams.append('--'.join(q[w_index:w_index+n]))\n",
    "            return q + ngrams\n",
    "        \n",
    "    docs = []\n",
    "    for doc in df.text:\n",
    "        doc = preprocess(doc).split()\n",
    "        docs.append(' '.join(add_ngram(doc, n_gram_max)))\n",
    "    \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "888047de-806e-4ad2-9fff-18b4d6583d30",
    "_uuid": "150f9f6643e6753386b2021ac812ecc0cac66202",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_count = 2\n",
    "\n",
    "docs = create_docs(df)\n",
    "tokenizer = Tokenizer(lower=False, filters='')\n",
    "tokenizer.fit_on_texts(docs)\n",
    "num_words = sum([1 for _, v in tokenizer.word_counts.items() if v >= min_count])\n",
    "\n",
    "tokenizer = Tokenizer(num_words=num_words, lower=False, filters='')\n",
    "tokenizer.fit_on_texts(docs)\n",
    "docs = tokenizer.texts_to_sequences(docs)\n",
    "\n",
    "maxlen = 256\n",
    "\n",
    "docs = pad_sequences(sequences=docs, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f9ebc033-2a26-4656-9472-8990c1a27c79",
    "_uuid": "b9e353b548b0dfbd4b42a40d8a2643efeb359a20"
   },
   "source": [
    "# **2. Model: FastText by Keras**\n",
    "\n",
    "FastText is very fast and strong baseline algorithm for text classification based on Continuous Bag-of-Words model a.k.a Word2vec.\n",
    "\n",
    "FastText contains only three layers:\n",
    "\n",
    "1. Embeddings layer: Input words (and word n-grams) are all words in a sentence/document\n",
    "2. Mean/AveragePooling Layer: Taking average vector of Embedding vectors\n",
    "3. Softmax layer\n",
    "\n",
    "There are some implementations of FastText:\n",
    "\n",
    "- Original library provided by Facebook AI research: https://github.com/facebookresearch/fastText\n",
    "- Keras: https://github.com/fchollet/keras/blob/master/examples/imdb_fasttext.py\n",
    "- Gensim: https://radimrehurek.com/gensim/models/wrappers/fasttext.html\n",
    "\n",
    "Original Paper: https://arxiv.org/abs/1607.01759 : More detail information about fastText classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "636eb75e-6fba-413e-996d-1395609b422c",
    "_uuid": "8b56b2ef90e519b939b7bf9ec5a146f749807b02"
   },
   "source": [
    "# My FastText parameters are:\n",
    "\n",
    "- The dimension of word vector is 20\n",
    "- Optimizer is `Adam`\n",
    "- Inputs are words and word bi-grams\n",
    "  - you can change this parameter by passing the max n-gram size to argument of `create_docs` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "393d1ddb-0a87-42a3-8575-53ff7abff1da",
    "_uuid": "bba1d1a6416876e74ed688f56e4d5bc4990ec12a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_dim = np.max(docs) + 1\n",
    "embedding_dims = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "2e3e1e3e-22f4-4727-ba6c-67f7b3e80d2f",
    "_uuid": "e6c16572e6b32923af39dfd29467e32b52561bb1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(embedding_dims=20, optimizer='adam'):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=input_dim, output_dim=embedding_dims))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "0db889db-0b3e-4025-8847-e3eb5f853f37",
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "_uuid": "22e57e010206a3044adf7b82160c7c3ca78030f8",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/25\n",
      "15663/15663 [==============================] - 40s 3ms/step - loss: 1.0675 - acc: 0.4077 - val_loss: 1.0298 - val_acc: 0.4545\n",
      "Epoch 2/25\n",
      "15663/15663 [==============================] - 40s 3ms/step - loss: 0.9330 - acc: 0.6121 - val_loss: 0.8592 - val_acc: 0.7163\n",
      "Epoch 3/25\n",
      "15663/15663 [==============================] - 40s 3ms/step - loss: 0.7250 - acc: 0.7874 - val_loss: 0.7072 - val_acc: 0.7574\n",
      "Epoch 4/25\n",
      "15663/15663 [==============================] - 40s 3ms/step - loss: 0.5657 - acc: 0.8475 - val_loss: 0.6096 - val_acc: 0.7870\n",
      "Epoch 5/25\n",
      "15663/15663 [==============================] - 38s 2ms/step - loss: 0.4523 - acc: 0.8801 - val_loss: 0.5402 - val_acc: 0.8041\n",
      "Epoch 6/25\n",
      "15663/15663 [==============================] - 37s 2ms/step - loss: 0.3663 - acc: 0.9068 - val_loss: 0.4877 - val_acc: 0.8195\n",
      "Epoch 7/25\n",
      "15663/15663 [==============================] - 37s 2ms/step - loss: 0.2987 - acc: 0.9283 - val_loss: 0.4506 - val_acc: 0.8299\n",
      "Epoch 8/25\n",
      "15663/15663 [==============================] - 37s 2ms/step - loss: 0.2449 - acc: 0.9446 - val_loss: 0.4190 - val_acc: 0.8450\n",
      "Epoch 9/25\n",
      "15663/15663 [==============================] - 36s 2ms/step - loss: 0.2010 - acc: 0.9580 - val_loss: 0.3980 - val_acc: 0.8463\n",
      "Epoch 10/25\n",
      "15663/15663 [==============================] - 36s 2ms/step - loss: 0.1654 - acc: 0.9666 - val_loss: 0.3851 - val_acc: 0.8488\n",
      "Epoch 11/25\n",
      "15663/15663 [==============================] - 36s 2ms/step - loss: 0.1357 - acc: 0.9744 - val_loss: 0.3657 - val_acc: 0.8593\n",
      "Epoch 12/25\n",
      "15663/15663 [==============================] - 37s 2ms/step - loss: 0.1118 - acc: 0.9804 - val_loss: 0.3588 - val_acc: 0.8634\n",
      "Epoch 13/25\n",
      "15663/15663 [==============================] - 42s 3ms/step - loss: 0.0920 - acc: 0.9850 - val_loss: 0.3477 - val_acc: 0.8693\n",
      "Epoch 14/25\n",
      "15663/15663 [==============================] - 41s 3ms/step - loss: 0.0762 - acc: 0.9876 - val_loss: 0.3429 - val_acc: 0.8685\n",
      "Epoch 15/25\n",
      "15663/15663 [==============================] - 40s 3ms/step - loss: 0.0629 - acc: 0.9906 - val_loss: 0.3409 - val_acc: 0.8685\n",
      "Epoch 16/25\n",
      "15663/15663 [==============================] - 40s 3ms/step - loss: 0.0522 - acc: 0.9923 - val_loss: 0.3413 - val_acc: 0.8703\n",
      "Epoch 17/25\n",
      "15663/15663 [==============================] - 39s 3ms/step - loss: 0.0433 - acc: 0.9937 - val_loss: 0.3459 - val_acc: 0.8667\n"
     ]
    }
   ],
   "source": [
    "epochs = 25\n",
    "x_train, x_test, y_train, y_test = train_test_split(docs, y, test_size=0.2)\n",
    "\n",
    "model = create_model()\n",
    "hist = model.fit(x_train, y_train,\n",
    "                 batch_size=16,\n",
    "                 validation_data=(x_test, y_test),\n",
    "                 epochs=epochs,\n",
    "                 callbacks=[EarlyStopping(patience=2, monitor='val_loss')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a3ea919b-2a46-4f0e-ada1-6d443e743464",
    "_uuid": "df6292660b8f3bed2ad66f78e346fc8cf6d51c74"
   },
   "source": [
    "## Result\n",
    "\n",
    "- Best `val_loss` is 0.3409\n",
    "- Best `val_acc` is 0.8700"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d7251e64-4dd0-4c80-a027-f14007a96110",
    "_uuid": "b770a07393bc7dbc971da8a1562e16f801519822"
   },
   "source": [
    "## **2.1** Change Preprocessings\n",
    "\n",
    "Next, I change some parameters and preprocessings to improve fastText model.\n",
    "\n",
    "###  **2.1.1 Do lower case**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "222af43b-b334-4f8b-ac20-48e62e1f92e7",
    "_uuid": "b97d1246094058bfbe4352cfa2c272e4a2b1ce8b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs = create_docs(df)\n",
    "tokenizer = Tokenizer(lower=True, filters='')\n",
    "tokenizer.fit_on_texts(docs)\n",
    "num_words = sum([1 for _, v in tokenizer.word_counts.items() if v >= min_count])\n",
    "\n",
    "tokenizer = Tokenizer(num_words=num_words, lower=True, filters='')\n",
    "tokenizer.fit_on_texts(docs)\n",
    "docs = tokenizer.texts_to_sequences(docs)\n",
    "\n",
    "maxlen = 256\n",
    "\n",
    "docs = pad_sequences(sequences=docs, maxlen=maxlen)\n",
    "\n",
    "input_dim = np.max(docs) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "0dac0cbe-6081-44d6-a98d-7a740972dfb9",
    "_kg_hide-output": true,
    "_uuid": "38bd08048747160016216f5ab8c8ccdf18295f21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/25\n",
      "15663/15663 [==============================] - 40s 3ms/step - loss: 1.0664 - acc: 0.4097 - val_loss: 1.0258 - val_acc: 0.4221\n",
      "Epoch 2/25\n",
      "15663/15663 [==============================] - 39s 3ms/step - loss: 0.9242 - acc: 0.6185 - val_loss: 0.8485 - val_acc: 0.6813\n",
      "Epoch 3/25\n",
      "15663/15663 [==============================] - 42s 3ms/step - loss: 0.7203 - acc: 0.7862 - val_loss: 0.6999 - val_acc: 0.7638\n",
      "Epoch 4/25\n",
      "15663/15663 [==============================] - 40s 3ms/step - loss: 0.5662 - acc: 0.8396 - val_loss: 0.5998 - val_acc: 0.7962\n",
      "Epoch 5/25\n",
      "15663/15663 [==============================] - 39s 2ms/step - loss: 0.4558 - acc: 0.8771 - val_loss: 0.5334 - val_acc: 0.8105\n",
      "Epoch 6/25\n",
      "15663/15663 [==============================] - 40s 3ms/step - loss: 0.3723 - acc: 0.9021 - val_loss: 0.4812 - val_acc: 0.8325\n",
      "Epoch 7/25\n",
      "15663/15663 [==============================] - 38s 2ms/step - loss: 0.3059 - acc: 0.9240 - val_loss: 0.4437 - val_acc: 0.8412\n",
      "Epoch 8/25\n",
      "15663/15663 [==============================] - 39s 2ms/step - loss: 0.2528 - acc: 0.9387 - val_loss: 0.4140 - val_acc: 0.8470\n",
      "Epoch 9/25\n",
      "15663/15663 [==============================] - 39s 2ms/step - loss: 0.2088 - acc: 0.9538 - val_loss: 0.3933 - val_acc: 0.8493\n",
      "Epoch 10/25\n",
      "15663/15663 [==============================] - 38s 2ms/step - loss: 0.1732 - acc: 0.9635 - val_loss: 0.3736 - val_acc: 0.8555\n",
      "Epoch 11/25\n",
      "15663/15663 [==============================] - 38s 2ms/step - loss: 0.1438 - acc: 0.9702 - val_loss: 0.3624 - val_acc: 0.8575\n",
      "Epoch 12/25\n",
      "15663/15663 [==============================] - 39s 2ms/step - loss: 0.1198 - acc: 0.9770 - val_loss: 0.3495 - val_acc: 0.8621\n",
      "Epoch 13/25\n",
      "15663/15663 [==============================] - 39s 2ms/step - loss: 0.0992 - acc: 0.9808 - val_loss: 0.3428 - val_acc: 0.8636\n",
      "Epoch 14/25\n",
      "15663/15663 [==============================] - 39s 3ms/step - loss: 0.0832 - acc: 0.9855 - val_loss: 0.3414 - val_acc: 0.8639\n",
      "Epoch 15/25\n",
      "15663/15663 [==============================] - 38s 2ms/step - loss: 0.0693 - acc: 0.9880 - val_loss: 0.3339 - val_acc: 0.8685\n",
      "Epoch 16/25\n",
      "15663/15663 [==============================] - 37s 2ms/step - loss: 0.0582 - acc: 0.9912 - val_loss: 0.3411 - val_acc: 0.8641\n",
      "Epoch 17/25\n",
      "15663/15663 [==============================] - 38s 2ms/step - loss: 0.0491 - acc: 0.9922 - val_loss: 0.3358 - val_acc: 0.8708\n"
     ]
    }
   ],
   "source": [
    "epochs = 25\n",
    "x_train, x_test, y_train, y_test = train_test_split(docs, y, test_size=0.2)\n",
    "\n",
    "model = create_model()\n",
    "hist = model.fit(x_train, y_train,\n",
    "                 batch_size=16,\n",
    "                 validation_data=(x_test, y_test),\n",
    "                 epochs=epochs,\n",
    "                 callbacks=[EarlyStopping(patience=2, monitor='val_loss')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f4369b68-6970-45c8-b325-c022071371df",
    "_uuid": "e5b831d81630c65040bc4f38e86236e74dec07f6"
   },
   "source": [
    "## Result\n",
    "\n",
    "- Best `val_loss` is 0.3129\n",
    "- Best `val_acc` is 0.8787"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "701f4d88-0d63-484d-bda7-7b776ab8639b",
    "_uuid": "49958cf9ef0fe2dcb08fa6957b8fd976e12055a4"
   },
   "source": [
    "### **2.1.2 Use less words**\n",
    "I decrease the length of sentence to train model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "df43b7b0-78b3-4335-9a37-94ebec0ea16d",
    "_uuid": "6f04876723372e76839e039d8236581048e980e6",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs = create_docs(df)\n",
    "tokenizer = Tokenizer(lower=True, filters='')\n",
    "tokenizer.fit_on_texts(docs)\n",
    "num_words = sum([1 for _, v in tokenizer.word_counts.items() if v >= min_count])\n",
    "\n",
    "tokenizer = Tokenizer(num_words=num_words, lower=True, filters='')\n",
    "tokenizer.fit_on_texts(docs)\n",
    "docs = tokenizer.texts_to_sequences(docs)\n",
    "\n",
    "maxlen = 128\n",
    "\n",
    "docs = pad_sequences(sequences=docs, maxlen=maxlen)\n",
    "\n",
    "input_dim = np.max(docs) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "26a3a554-6fe4-4aaa-936a-9e7a47e7ef82",
    "_uuid": "9f742edfefc7e60aff192d425a42158b8d23b0b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/25\n",
      "15663/15663 [==============================] - 38s 2ms/step - loss: 1.0360 - acc: 0.4549 - val_loss: 0.9477 - val_acc: 0.5723\n",
      "Epoch 2/25\n",
      "15663/15663 [==============================] - 39s 3ms/step - loss: 0.7791 - acc: 0.7407 - val_loss: 0.7118 - val_acc: 0.7406\n",
      "Epoch 3/25\n",
      "15663/15663 [==============================] - 39s 3ms/step - loss: 0.5466 - acc: 0.8446 - val_loss: 0.5827 - val_acc: 0.7893\n",
      "Epoch 4/25\n",
      "15663/15663 [==============================] - 41s 3ms/step - loss: 0.4029 - acc: 0.8922 - val_loss: 0.5080 - val_acc: 0.8098\n",
      "Epoch 5/25\n",
      "15663/15663 [==============================] - 40s 3ms/step - loss: 0.3053 - acc: 0.9240 - val_loss: 0.4545 - val_acc: 0.8271\n",
      "Epoch 6/25\n",
      "15663/15663 [==============================] - 40s 3ms/step - loss: 0.2333 - acc: 0.9461 - val_loss: 0.4189 - val_acc: 0.8368\n",
      "Epoch 7/25\n",
      "15663/15663 [==============================] - 37s 2ms/step - loss: 0.1794 - acc: 0.9631 - val_loss: 0.3957 - val_acc: 0.8430\n",
      "Epoch 8/25\n",
      "15663/15663 [==============================] - 36s 2ms/step - loss: 0.1384 - acc: 0.9728 - val_loss: 0.3757 - val_acc: 0.8537\n",
      "Epoch 9/25\n",
      "15663/15663 [==============================] - 38s 2ms/step - loss: 0.1075 - acc: 0.9804 - val_loss: 0.3640 - val_acc: 0.8547\n",
      "Epoch 10/25\n",
      "15663/15663 [==============================] - 36s 2ms/step - loss: 0.0829 - acc: 0.9859 - val_loss: 0.3632 - val_acc: 0.8514\n",
      "Epoch 11/25\n",
      "15663/15663 [==============================] - 37s 2ms/step - loss: 0.0644 - acc: 0.9897 - val_loss: 0.3561 - val_acc: 0.8547\n",
      "Epoch 12/25\n",
      "15663/15663 [==============================] - 36s 2ms/step - loss: 0.0501 - acc: 0.9934 - val_loss: 0.3558 - val_acc: 0.8552\n",
      "Epoch 13/25\n",
      "15663/15663 [==============================] - 37s 2ms/step - loss: 0.0391 - acc: 0.9949 - val_loss: 0.3586 - val_acc: 0.8578\n",
      "Epoch 14/25\n",
      "15663/15663 [==============================] - 36s 2ms/step - loss: 0.0306 - acc: 0.9958 - val_loss: 0.3708 - val_acc: 0.8532\n"
     ]
    }
   ],
   "source": [
    "epochs = 25\n",
    "x_train, x_test, y_train, y_test = train_test_split(docs, y, test_size=0.2)\n",
    "\n",
    "model = create_model()\n",
    "hist = model.fit(x_train, y_train,\n",
    "                 batch_size=16,\n",
    "                 validation_data=(x_test, y_test),\n",
    "                 epochs=epochs,\n",
    "                 callbacks=[EarlyStopping(patience=2, monitor='val_loss')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "52a07f6b-2a3a-401a-aee3-5421c53f0588",
    "_uuid": "f5d9f261c1befdb6b05bbd39536c78ad74396d90"
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('input/test.csv')\n",
    "docs = create_docs(test_df)\n",
    "docs = tokenizer.texts_to_sequences(docs)\n",
    "docs = pad_sequences(sequences=docs, maxlen=maxlen)\n",
    "y = model.predict_proba(docs)\n",
    "\n",
    "result = pd.read_csv('input/sample_submission.csv')\n",
    "for a, i in a2c.items():\n",
    "    result[a] = y[:, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "409b8663-bbf5-4757-99c1-40010264de04",
    "_uuid": "eaf5b87353d9a1e7367def64b453555c23d24e7a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to_submit=result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
