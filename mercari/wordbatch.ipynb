{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on Bojan -> https://www.kaggle.com/tunguz/more-effective-ridge-lgbm-script-lb-0-44944\n",
    "# and Nishant -> https://www.kaggle.com/nishkgp/more-improved-ridge-2-lgbm\n",
    "\n",
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "\n",
    "import sys\n",
    "\n",
    "#Add https://www.kaggle.com/anttip/wordbatch to your kernel Data Sources, \n",
    "#until Kaggle admins fix the wordbatch pip package installation\n",
    "sys.path.insert(0, '../input/wordbatch/wordbatch/')\n",
    "import wordbatch\n",
    "\n",
    "from wordbatch.extractors import WordBag, WordHash\n",
    "from wordbatch.models import FTRL, FM_FTRL\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_BRANDS = 4500\n",
    "NUM_CATEGORIES = 1250\n",
    "\n",
    "develop = False\n",
    "# develop= True\n",
    "\n",
    "def rmsle(y, y0):\n",
    "    assert len(y) == len(y0)\n",
    "    return np.sqrt(np.mean(np.power(np.log1p(y) - np.log1p(y0), 2)))\n",
    "\n",
    "\n",
    "def split_cat(text):\n",
    "    try:\n",
    "        return text.split(\"/\")\n",
    "    except:\n",
    "        return (\"No Label\", \"No Label\", \"No Label\")\n",
    "\n",
    "\n",
    "def handle_missing_inplace(dataset):\n",
    "    dataset['general_cat'].fillna(value='missing', inplace=True)\n",
    "    dataset['subcat_1'].fillna(value='missing', inplace=True)\n",
    "    dataset['subcat_2'].fillna(value='missing', inplace=True)\n",
    "    dataset['brand_name'].fillna(value='missing', inplace=True)\n",
    "    dataset['item_description'].fillna(value='missing', inplace=True)\n",
    "\n",
    "\n",
    "def cutting(dataset):\n",
    "    pop_brand = dataset['brand_name'].value_counts().loc[lambda x: x.index != 'missing'].index[:NUM_BRANDS]\n",
    "    dataset.loc[~dataset['brand_name'].isin(pop_brand), 'brand_name'] = 'missing'\n",
    "    pop_category1 = dataset['general_cat'].value_counts().loc[lambda x: x.index != 'missing'].index[:NUM_CATEGORIES]\n",
    "    pop_category2 = dataset['subcat_1'].value_counts().loc[lambda x: x.index != 'missing'].index[:NUM_CATEGORIES]\n",
    "    pop_category3 = dataset['subcat_2'].value_counts().loc[lambda x: x.index != 'missing'].index[:NUM_CATEGORIES]\n",
    "    dataset.loc[~dataset['general_cat'].isin(pop_category1), 'general_cat'] = 'missing'\n",
    "    dataset.loc[~dataset['subcat_1'].isin(pop_category2), 'subcat_1'] = 'missing'\n",
    "    dataset.loc[~dataset['subcat_2'].isin(pop_category3), 'subcat_2'] = 'missing'\n",
    "\n",
    "\n",
    "def to_categorical(dataset):\n",
    "    dataset['general_cat'] = dataset['general_cat'].astype('category')\n",
    "    dataset['subcat_1'] = dataset['subcat_1'].astype('category')\n",
    "    dataset['subcat_2'] = dataset['subcat_2'].astype('category')\n",
    "    dataset['item_condition_id'] = dataset['item_condition_id'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-02-01 08:51:41\n",
      "[8.452430248260498] Finished to load data\n",
      "Train shape:  (1482535, 8)\n",
      "Test shape:  (693359, 7)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = {x: 1 for x in stopwords.words('english')}\n",
    "non_alphanums = re.compile(u'[^A-Za-z0-9]+')\n",
    "\n",
    "\n",
    "def normalize_text(text):\n",
    "    return u\" \".join(\n",
    "        [x for x in [y for y in non_alphanums.sub(' ', text).lower().strip().split(\" \")] \\\n",
    "         if len(x) > 1 and x not in stopwords])\n",
    "\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "from time import gmtime, strftime\n",
    "print(strftime(\"%Y-%m-%d %H:%M:%S\", gmtime()))\n",
    "\n",
    "# if 1 == 1:\n",
    "train = pd.read_table('input/train.tsv', engine='c')\n",
    "test = pd.read_table('input/test.tsv', engine='c')\n",
    "\n",
    "#train = pd.read_table('../input/train.tsv', engine='c')\n",
    "#test = pd.read_table('../input/test.tsv', engine='c')\n",
    "\n",
    "print('[{}] Finished to load data'.format(time.time() - start_time))\n",
    "print('Train shape: ', train.shape)\n",
    "print('Test shape: ', test.shape)\n",
    "nrow_test = train.shape[0]  # -dftt.shape[0]\n",
    "dftt = train[(train.price < 1.0)]\n",
    "train = train.drop(train[(train.price < 1.0)].index)\n",
    "del dftt['price']\n",
    "nrow_train = train.shape[0]\n",
    "# print(nrow_train, nrow_test)\n",
    "y = np.log1p(train[\"price\"])\n",
    "merge: pd.DataFrame = pd.concat([train, dftt, test])\n",
    "submission: pd.DataFrame = test[['test_id']]\n",
    "\n",
    "del train\n",
    "del test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17.98873805999756] Split categories completed.\n",
      "[18.780839443206787] Handle missing completed.\n",
      "[39.215068101882935] Cut completed.\n",
      "[41.30762028694153] Convert categorical completed\n",
      "Normalize text\n",
      "Parallelization fail. Method: multiprocessing Task: <function batch_normalize_texts at 0x0000022F245E19D8>\n",
      "Retrying, attempt: 1 timeout limit: 1200 seconds\n",
      "Parallelization fail. Method: multiprocessing Task: <function batch_normalize_texts at 0x0000022F245E19D8>\n",
      "Retrying, attempt: 2 timeout limit: 2400 seconds\n",
      "Parallelization fail. Method: multiprocessing Task: <function batch_normalize_texts at 0x0000022F245E19D8>\n",
      "Retrying, attempt: 3 timeout limit: 4800 seconds\n"
     ]
    }
   ],
   "source": [
    "merge['general_cat'], merge['subcat_1'], merge['subcat_2'] = \\\n",
    "    zip(*merge['category_name'].apply(lambda x: split_cat(x)))\n",
    "merge.drop('category_name', axis=1, inplace=True)\n",
    "print('[{}] Split categories completed.'.format(time.time() - start_time))\n",
    "\n",
    "handle_missing_inplace(merge)\n",
    "print('[{}] Handle missing completed.'.format(time.time() - start_time))\n",
    "\n",
    "cutting(merge)\n",
    "print('[{}] Cut completed.'.format(time.time() - start_time))\n",
    "\n",
    "to_categorical(merge)\n",
    "print('[{}] Convert categorical completed'.format(time.time() - start_time))\n",
    "\n",
    "wb = wordbatch.WordBatch(normalize_text, extractor=(WordBag, {\"hash_ngrams\": 2, \"hash_ngrams_weights\": [1.5, 1.0],\n",
    "                                                              \"hash_size\": 2 ** 29, \"norm\": None, \"tf\": 'binary',\n",
    "                                                              \"idf\": None,\n",
    "                                                              }), procs=8)\n",
    "wb.dictionary_freeze= True\n",
    "X_name = wb.fit_transform(merge['name'])\n",
    "del(wb)\n",
    "X_name = X_name[:, np.array(np.clip(X_name.getnnz(axis=0) - 1, 0, 1), dtype=bool)]\n",
    "print('[{}] Vectorize `name` completed.'.format(time.time() - start_time))\n",
    "\n",
    "wb = CountVectorizer()\n",
    "X_category1 = wb.fit_transform(merge['general_cat'])\n",
    "X_category2 = wb.fit_transform(merge['subcat_1'])\n",
    "X_category3 = wb.fit_transform(merge['subcat_2'])\n",
    "print('[{}] Count vectorize `categories` completed.'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# wb= wordbatch.WordBatch(normalize_text, extractor=(WordBag, {\"hash_ngrams\": 3, \"hash_ngrams_weights\": [1.0, 1.0, 0.5],\n",
    "wb = wordbatch.WordBatch(normalize_text, extractor=(WordBag, {\"hash_ngrams\": 2, \"hash_ngrams_weights\": [1.0, 1.0],\n",
    "                                                              \"hash_size\": 2 ** 28, \"norm\": \"l2\", \"tf\": 1.0,\n",
    "                                                              \"idf\": None})\n",
    "                         , procs=8)\n",
    "wb.dictionary_freeze= True\n",
    "X_description = wb.fit_transform(merge['item_description'])\n",
    "del(wb)\n",
    "X_description = X_description[:, np.array(np.clip(X_description.getnnz(axis=0) - 1, 0, 1), dtype=bool)]\n",
    "print('[{}] Vectorize `item_description` completed.'.format(time.time() - start_time))\n",
    "\n",
    "lb = LabelBinarizer(sparse_output=True)\n",
    "X_brand = lb.fit_transform(merge['brand_name'])\n",
    "print('[{}] Label binarize `brand_name` completed.'.format(time.time() - start_time))\n",
    "\n",
    "X_dummies = csr_matrix(pd.get_dummies(merge[['item_condition_id', 'shipping']],\n",
    "                                      sparse=True).values)\n",
    "print('[{}] Get dummies on `item_condition_id` and `shipping` completed.'.format(time.time() - start_time))\n",
    "print(X_dummies.shape, X_description.shape, X_brand.shape, X_category1.shape, X_category2.shape, X_category3.shape,\n",
    "      X_name.shape)\n",
    "sparse_merge = hstack((X_dummies, X_description, X_brand, X_category1, X_category2, X_category3, X_name)).tocsr()\n",
    "\n",
    "print('[{}] Create sparse merge completed'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
